# Big-Data-Analytics-with-Apache-Spark-SQL
Analyzing Big Data with Spark SQL 

Working with SQL at Scale - Spark SQL ğŸš€ğŸ“Š

Overview

This repository contains a comprehensive tutorial on using Spark SQL for large-scale data processing and querying. It demonstrates how to leverage Spark SQL to work with structured data, perform complex queries, and gain insights from large datasets. The tutorial uses the KDD Cup 1999 dataset, a well-known dataset for network intrusion detection. ğŸŒğŸ”

Features

Data Loading: Import data from Parquet files and Hive tables. ğŸ“¥

SQL Queries: Run SQL queries over imported data and existing RDDs. ğŸ“Š

Data Export: Write RDDs out to Hive tables or Parquet files. ğŸ“¤

Optimized Performance: Utilize Spark SQL's cost-based optimizer and columnar storage for fast queries. âš¡

Scalability: Scale to thousands of nodes and multi-hour queries with full fault tolerance. ğŸŒ

Results

Protocol Type Aggregation: Count of connections based on protocol type (TCP, UDP, ICMP). ğŸ“ˆ

Attack Type Aggregation: Count of connections based on attack types (e.g., smurf, neptune, normal). ğŸš¨

Connection Stats: Detailed statistics on connections, including mean duration, failed logins, and root accesses. ğŸ“Š

Contributing :
Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request. ğŸ¤
